{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gKBLu6htCFTh"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix, csr_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import math\n",
        "import os # For checking file existence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Set the paths to your data files\n",
        "# IMPORTANT: Replace these with the actual paths to your files\n",
        "data_dir = './' # Assume files are in the current directory\n",
        "train_data_file = os.path.join(data_dir, 'trainData.txt')\n",
        "test_data_file = os.path.join(data_dir, 'testData.txt')\n",
        "train_label_file = os.path.join(data_dir, 'trainLabel.txt')\n",
        "test_label_file = os.path.join(data_dir, 'testLabel.txt')\n",
        "vocab_file = os.path.join(data_dir, 'words.txt')\n"
      ],
      "metadata": {
        "id": "Zp67z788KPMK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Function to Load Data ---\n",
        "def load_data(data_file, label_file, num_docs, num_words):\n",
        "    \"\"\"Loads document data and labels into a sparse matrix and label array.\"\"\"\n",
        "    print(f\"Loading data from {data_file} and {label_file}...\")\n",
        "\n",
        "    # Initialize a sparse matrix (LIL format is efficient for construction)\n",
        "    # Documents are rows, words are columns\n",
        "    # Using 1-based indexing from files, so size is num_docs x num_words\n",
        "    X = lil_matrix((num_docs, num_words), dtype=int)\n",
        "    y = np.zeros(num_docs, dtype=int)\n",
        "\n",
        "    # Read data file (docId wordId)\n",
        "    doc_ids_present = set()\n",
        "    try:\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                doc_id, word_id = map(int, line.strip().split())\n",
        "                # Adjust to 0-based index for matrix\n",
        "                if 1 <= doc_id <= num_docs and 1 <= word_id <= num_words:\n",
        "                    X[doc_id - 1, word_id - 1] = 1 # Binary feature: word is present\n",
        "                    doc_ids_present.add(doc_id)\n",
        "                else:\n",
        "                    print(f\"Warning: Out of bounds index found in {data_file}: doc_id={doc_id}, word_id={word_id}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data file not found at {data_file}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {data_file}: {e}\")\n",
        "        return None, None\n",
        "    # Read label file (label per line, line number corresponds to docId)\n",
        "    try:\n",
        "        with open(label_file, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                doc_id = i + 1 # Line number is docId (1-based)\n",
        "                if doc_id in doc_ids_present: # Only load labels for docs present in data file\n",
        "                     # Ensure the index is within the bounds of y\n",
        "                    if 0 <= doc_id - 1 < num_docs:\n",
        "                         y[doc_id - 1] = int(line.strip())\n",
        "                    else:\n",
        "                         print(f\"Warning: doc_id {doc_id} from {label_file} out of bounds for y array (size {num_docs}).\")\n",
        "\n",
        "                # Handle cases where label file might have more lines than docs in data file\n",
        "                elif doc_id > num_docs:\n",
        "                    # print(f\"Warning: More labels in {label_file} than specified num_docs ({num_docs}). Stopping label read.\")\n",
        "                    break # Stop if we exceed expected number of docs\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Label file not found at {label_file}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {label_file}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loaded {X.shape[0]} documents and {len(np.unique(y))} labels.\")\n",
        "    # Convert to CSR format for efficient calculations\n",
        "    return X.tocsr(), y\n"
      ],
      "metadata": {
        "id": "wZgmXR0hKBAe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Script ---\n",
        "# 1. Load Vocabulary\n",
        "print(\"Loading vocabulary...\")\n",
        "words = []\n",
        "try:\n",
        "    with open(vocab_file, 'r') as f:\n",
        "        words = [line.strip() for line in f]\n",
        "    num_words = len(words)\n",
        "    if num_words == 0:\n",
        "        raise ValueError(\"Vocabulary file is empty.\")\n",
        "    print(f\"Vocabulary size: {num_words} words.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Vocabulary file not found at {vocab_file}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading {vocab_file}: {e}\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "zhJZvUHxJyaB",
        "outputId": "02cefe1e-893a-4546-bec6-d4d23268d3e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vocabulary...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-80e452d7be73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_file' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-80e452d7be73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error reading {vocab_file}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Determine Number of Documents (using label files is often reliable)\n",
        "print(\"Determining number of documents...\")\n",
        "try:\n",
        "    with open(train_label_file, 'r') as f:\n",
        "        num_train_docs = sum(1 for _ in f)\n",
        "    with open(test_label_file, 'r') as f:\n",
        "        num_test_docs = sum(1 for _ in f)\n",
        "    print(f\"Found {num_train_docs} training documents and {num_test_docs} testing documents.\")\n",
        "    if num_train_docs == 0 or num_test_docs == 0:\n",
        "        raise ValueError(\"Label files indicate zero documents.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Label file(s) not found. Cannot determine document counts.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading label files: {e}\")\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW49qtGaJRo7",
        "outputId": "3c2fc4e5-a7b4-4706-a620-dd2a2b0cadde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determining number of documents...\n",
            "Error reading label files: name 'train_label_file' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load Training and Testing Data\n",
        "X_train_sparse, y_train = load_data(train_data_file, train_label_file, num_train_docs, num_words)\n",
        "X_test_sparse, y_test = load_data(test_data_file, test_label_file, num_test_docs, num_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "r7-Z0LeWJPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if loading was successful\n",
        "if X_train_sparse is None or X_test_sparse is None:\n",
        "    print(\"Failed to load data. Exiting.\")\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "YjJrpPZLI3Zr",
        "outputId": "89bd1e09-194c-4af0-bf10-f1494e0fcf35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_sparse' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d85f58233836>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if loading was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mX_train_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX_test_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to load data. Exiting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_sparse' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part a: Naïve Bayes ---\n",
        "print(\"\\n--- Part a: Gaussian Naïve Bayes ---\")\n"
      ],
      "metadata": {
        "id": "ipi2OFW0IzV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i. Train GaussianNB and find discriminative words\n",
        "print(\"Training Gaussian Naive Bayes classifier...\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_dense, y_train)\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "AbqDFpa1IxW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate discriminative scores\n",
        "# GaussianNB stores mean (theta_) and variance (var_) for each feature per class.\n",
        "# Shape of theta_ and var_ is (n_classes, n_features)\n",
        "# Labels are 1 and 2, so indices are 0 and 1 after fitting.\n",
        "means_label1 = gnb.theta_[0, :]\n",
        "vars_label1 = gnb.var_[0, :]\n",
        "means_label2 = gnb.theta_[1, :]\n",
        "vars_label2 = gnb.var_[1, :]\n"
      ],
      "metadata": {
        "id": "tylRalp1IWwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1e-9\n",
        "vars_label1 = np.maximum(vars_label1, epsilon)\n",
        "vars_label2 = np.maximum(vars_label2, epsilon)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RcC9oJpgITkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate log probability density at x=1 (presence of word) for each word and class\n",
        "# PDF of Normal(mu, sigma^2) is (1 / sqrt(2*pi*sigma^2)) * exp(-(x-mu)^2 / (2*sigma^2))\n",
        "# Log PDF is -log(sqrt(2*pi*sigma^2)) - (x-mu)^2 / (2*sigma^2)\n",
        "# We evaluate at x=1\n",
        "x_val = 1\n",
        "log_prob_word_label1 = -0.5 * np.log(2 * np.pi * vars_label1) - ((x_val - means_label1)**2 / (2 * vars_label1))\n",
        "log_prob_word_label2 = -0.5 * np.log(2 * np.pi * vars_label2) - ((x_val - means_label2)**2 / (2 * vars_label2))\n",
        "\n"
      ],
      "metadata": {
        "id": "w1o6LaNnINow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the discriminative score\n",
        "discriminative_scores = np.abs(log_prob_word_label1 - log_prob_word_label2)"
      ],
      "metadata": {
        "id": "N8xMKyBJHueN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices of the top 10 discriminative words\n",
        "top_10_indices = np.argsort(discriminative_scores)[-10:][::-1] # Sort descending\n",
        "\n",
        "print(\"\\nTop 10 most discriminative word features (GaussianNB):\")\n",
        "print(\"Rank | Word          | Score ( |log P(w|L1) - log P(w|L2)| )\")\n",
        "print(\"-----|---------------|----------------------------------------\")\n",
        "for i, word_index in enumerate(top_10_indices):\n",
        "    word = words[word_index]\n",
        "    score = discriminative_scores[word_index]\n",
        "    print(f\"{i+1:<4} | {word:<13} | {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "BDqSjbsQHqkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opinion on features: (Requires manual inspection of the words)\n",
        "print(\"\\nOpinion: Inspect the words above. Are they strongly related to one topic (e.g., 'god', 'atheism', 'christian', 'religion' for alt.atheism vs. comp.graphics)? If so, they are likely good discriminative features.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vo42vJnWGzZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ii. Calculate and print accuracy\n",
        "print(\"\\nCalculating Naive Bayes accuracy...\")\n",
        "y_train_pred_gnb = gnb.predict(X_train_dense)\n",
        "y_test_pred_gnb = gnb.predict(X_test_dense)\n",
        "\n",
        "train_accuracy_gnb = accuracy_score(y_train, y_train_pred_gnb)\n",
        "test_accuracy_gnb = accuracy_score(y_test, y_test_pred_gnb)\n",
        "\n",
        "print(f\"Gaussian Naive Bayes Training Accuracy: {train_accuracy_gnb:.4f} ({train_accuracy_gnb*100:.2f}%)\")\n",
        "print(f\"Gaussian Naive Bayes Testing Accuracy:  {test_accuracy_gnb:.4f} ({test_accuracy_gnb*100:.2f}%)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Abe83vctGKEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part b: SVM ---\n",
        "print(\"\\n--- Part b: Support Vector Machine (SVM) ---\")\n"
      ],
      "metadata": {
        "id": "11pWUJMgF-Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Scaling data using MaxAbsScaler...\")\n",
        "scaler = MaxAbsScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
        "X_test_scaled = scaler.transform(X_test_sparse)\n",
        "print(\"Scaling complete.\")\n"
      ],
      "metadata": {
        "id": "NaRXpvZqF4EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate SVM with Linear Kernel\n",
        "print(\"\\nTraining SVM with Linear Kernel...\")\n",
        "svm_linear = SVC(kernel='linear', random_state=42) # Added random_state for reproducibility\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "print(\"Calculating Linear SVM accuracy...\")\n",
        "y_train_pred_svm_linear = svm_linear.predict(X_train_scaled)\n",
        "y_test_pred_svm_linear = svm_linear.predict(X_test_scaled)\n",
        "\n",
        "train_accuracy_svm_linear = accuracy_score(y_train, y_train_pred_svm_linear)\n",
        "test_accuracy_svm_linear = accuracy_score(y_test, y_test_pred_svm_linear)\n",
        "\n",
        "print(f\"Linear SVM Training Accuracy: {train_accuracy_svm_linear:.4f} ({train_accuracy_svm_linear*100:.2f}%)\")\n",
        "print(f\"Linear SVM Testing Accuracy:  {test_accuracy_svm_linear:.4f} ({test_accuracy_svm_linear*100:.2f}%)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tb6LpBhNF0kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate SVM with Polynomial Kernel\n",
        "print(\"\\nTraining SVM with Polynomial Kernel...\")\n",
        "svm_poly = SVC(kernel='poly', random_state=42) # Added random_state for reproducibility\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "print(\"Calculating Polynomial SVM accuracy...\")\n",
        "y_train_pred_svm_poly = svm_poly.predict(X_train_scaled)\n",
        "y_test_pred_svm_poly = svm_poly.predict(X_test_scaled)\n",
        "\n",
        "train_accuracy_svm_poly = accuracy_score(y_train, y_train_pred_svm_poly)\n",
        "test_accuracy_svm_poly = accuracy_score(y_test, y_test_pred_svm_poly)\n",
        "\n",
        "print(f\"Polynomial SVM Training Accuracy: {train_accuracy_svm_poly:.4f} ({train_accuracy_svm_poly*100:.2f}%)\")\n",
        "print(f\"Polynomial SVM Testing Accuracy:  {test_accuracy_svm_poly:.4f} ({test_accuracy_svm_poly*100:.2f}%)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JxjwNFZ6Fsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Decision Boundaries (Conceptual)\n",
        "print(\"\\nPlotting Decision Boundaries:\")\n",
        "print(\"Directly plotting decision boundaries for high-dimensional text data ({} features) is not feasible.\".format(num_words))\n",
        "print(\"To visualize, one typically reduces dimensionality (e.g., using PCA or t-SNE to 2D)\")\n",
        "print(\"and then plots the boundary in that reduced space. This provides an approximation.\")\n",
        "print(\"Code for plotting is omitted here but would involve:\")\n",
        "print(\"1. `from sklearn.decomposition import PCA`\")\n",
        "print(\"2. `pca = PCA(n_components=2)`\")\n",
        "print(\"3. `X_train_pca = pca.fit_transform(X_train_scaled.toarray())` (or dense if already converted)\")\n",
        "print(\"4. Train SVMs on `X_train_pca`\")\n",
        "print(\"5. Use a meshgrid and `svm.predict` to plot decision regions (like the scikit-learn example).\")\n",
        "\n",
        "print(\"\\n--- End of Question 2 ---\")"
      ],
      "metadata": {
        "id": "dal7jaaHFqal"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}